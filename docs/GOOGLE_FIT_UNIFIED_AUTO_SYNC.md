# ðŸ”„ Google Fit Unified Auto Sync - Consolidation Proposal

## ðŸŽ¯ Current Architecture (Inefficient)

### Two Separate Cron Jobs

**1. Token Refresh** - Every 15 minutes
```sql
-- Runs: refresh-all-google-tokens
-- Does: Refresh tokens expiring within 30 minutes
-- Time: Every 15 minutes (96 times/day)
```

**2. Data Sync** - Every 5 minutes (?)
```sql
-- Runs: sync-all-users-direct
-- Does: Sync data, checks token expiry inline
-- Time: Every 5 minutes (288 times/day)
```

### Problems

1. **Redundant Token Checks**
   - Token refresh checks expiry
   - Data sync also checks expiry
   - Same work done twice

2. **Race Conditions**
   - Token could expire between refresh job and sync job
   - Refresh at 10:00, token expires 10:14, sync fails at 10:15

3. **Extra API Calls**
   - Separate jobs = separate overhead
   - Could refresh token that won't be used for hours

4. **Complex Scheduling**
   - Two cron jobs to maintain
   - Two edge functions to deploy
   - Two monitoring points

---

## âœ… Proposed Solution: Unified Auto Sync

### Single Cron Job

**Combined: Token Refresh + Data Sync**
```sql
-- Runs: sync-all-google-fit (renamed/unified)
-- Does: 
--   1. Refresh expiring tokens
--   2. Sync data for all users
--   3. Return combined metrics
-- Time: Every 10 minutes (144 times/day)
```

### Benefits

1. âœ… **Atomic Operation**
   - Refresh token â†’ immediately use it
   - No race conditions

2. âœ… **Just-In-Time Refresh**
   - Only refresh tokens we're about to use
   - Fewer wasted API calls

3. âœ… **Simpler Architecture**
   - One cron job
   - One edge function
   - One monitoring point

4. âœ… **Better Performance**
   - Reduced overhead
   - Less database queries
   - Fewer edge function cold starts

---

## ðŸ”§ Implementation

### New Unified Function: `sync-all-google-fit`

```typescript
// supabase/functions/sync-all-google-fit/index.ts

import { serve } from "https://deno.land/std@0.168.0/http/server.ts";
import { createClient } from 'https://esm.sh/@supabase/supabase-js@2';
import { refreshToken } from '../_shared/google-token-manager.ts';
import { fetchDayData, storeDayData } from '../_shared/google-fit-sync-core.ts';

interface SyncResult {
  user_id: string;
  success: boolean;
  token_refreshed: boolean;
  days_synced: number;
  sessions_synced: number;
  error?: string;
}

serve(async (req) => {
  if (req.method === "OPTIONS") {
    return new Response(null, { headers: corsHeaders });
  }

  try {
    const supabase = createClient(
      Deno.env.get('SUPABASE_URL')!,
      Deno.env.get('SUPABASE_SERVICE_ROLE_KEY')!
    );

    // Parse options
    const { 
      daysBack = 7,  // Default to last 7 days
      batchSize = 10,  // Process 10 users at a time
      tokenRefreshThreshold = 30  // Refresh tokens expiring within 30 min
    } = await req.json().catch(() => ({}));

    console.log(`Starting unified sync: ${daysBack} days back, batch size ${batchSize}`);

    // Get all active tokens
    const { data: tokens, error: tokensError } = await supabase
      .from('google_tokens')
      .select('*')
      .eq('is_active', true)
      .order('user_id');

    if (tokensError || !tokens) {
      throw new Error('Failed to fetch tokens');
    }

    console.log(`Found ${tokens.length} active tokens`);

    // Process in batches to avoid overwhelming API
    const results: SyncResult[] = [];
    const batches = chunk(tokens, batchSize);

    for (let batchIndex = 0; batchIndex < batches.length; batchIndex++) {
      const batch = batches[batchIndex];
      console.log(`Processing batch ${batchIndex + 1}/${batches.length}`);

      // Process batch in parallel
      const batchResults = await Promise.all(
        batch.map(token => syncUserData(supabase, token, {
          daysBack,
          tokenRefreshThreshold
        }))
      );

      results.push(...batchResults);

      // Rate limiting between batches
      if (batchIndex < batches.length - 1) {
        await delay(1000);  // 1 second between batches
      }
    }

    // Summarize results
    const summary = {
      total_users: tokens.length,
      successful_syncs: results.filter(r => r.success).length,
      failed_syncs: results.filter(r => !r.success).length,
      tokens_refreshed: results.filter(r => r.token_refreshed).length,
      total_days_synced: results.reduce((sum, r) => sum + (r.days_synced || 0), 0),
      total_sessions_synced: results.reduce((sum, r) => sum + (r.sessions_synced || 0), 0),
      errors: results.filter(r => !r.success).map(r => ({
        user_id: r.user_id,
        error: r.error
      }))
    };

    console.log('Sync complete:', summary);

    // Log to metrics table
    try {
      await supabase.from('google_fit_sync_metrics').insert({
        sync_type: 'auto',
        users_processed: tokens.length,
        successful: summary.successful_syncs,
        failed: summary.failed_syncs,
        tokens_refreshed: summary.tokens_refreshed,
        duration_ms: null,  // Could track this
        details: summary
      });
    } catch (logError) {
      console.warn('Failed to log metrics:', logError);
    }

    return new Response(JSON.stringify(summary), {
      headers: { ...corsHeaders, 'Content-Type': 'application/json' }
    });

  } catch (error) {
    console.error('Unified sync error:', error);
    return new Response(JSON.stringify({ 
      error: error.message 
    }), {
      status: 500,
      headers: { ...corsHeaders, 'Content-Type': 'application/json' }
    });
  }
});

/**
 * Sync data for a single user
 * Handles token refresh if needed
 */
async function syncUserData(
  supabase: any,
  token: any,
  options: { daysBack: number; tokenRefreshThreshold: number }
): Promise<SyncResult> {
  const result: SyncResult = {
    user_id: token.user_id,
    success: false,
    token_refreshed: false,
    days_synced: 0,
    sessions_synced: 0
  };

  try {
    // Check if token needs refresh
    const expiresAt = new Date(token.expires_at);
    const now = new Date();
    const minutesUntilExpiry = (expiresAt.getTime() - now.getTime()) / (1000 * 60);

    let accessToken = token.access_token;

    // Refresh token if expiring soon
    if (minutesUntilExpiry < options.tokenRefreshThreshold) {
      console.log(`Token expiring in ${minutesUntilExpiry.toFixed(0)} min for user ${token.user_id}, refreshing...`);
      
      try {
        const refreshed = await refreshToken(supabase, token);
        accessToken = refreshed.access_token;
        result.token_refreshed = true;
        console.log(`Token refreshed for user ${token.user_id}`);
      } catch (refreshError) {
        console.error(`Token refresh failed for user ${token.user_id}:`, refreshError);
        // Mark token as inactive
        await supabase
          .from('google_tokens')
          .update({ is_active: false })
          .eq('id', token.id);
        
        throw new Error('Token refresh failed');
      }
    }

    // Sync data for last N days
    const today = new Date();
    const sessionsTotal = [];

    for (let i = 0; i < options.daysBack; i++) {
      const targetDate = new Date(today);
      targetDate.setDate(targetDate.getDate() - i);
      
      const dateStr = targetDate.toISOString().split('T')[0];

      try {
        // Fetch day data using shared core
        const startOfDay = new Date(targetDate.setHours(0, 0, 0, 0));
        const endOfDay = new Date(targetDate.setHours(23, 59, 59, 999));

        const dayData = await fetchDayData(accessToken, startOfDay, endOfDay);

        // Store data using shared core
        await storeDayData(supabase, token.user_id, dateStr, dayData);

        result.days_synced++;
        sessionsTotal.push(...(dayData.sessions || []));

      } catch (dayError) {
        console.warn(`Failed to sync ${dateStr} for user ${token.user_id}:`, dayError);
        // Continue with other days
      }

      // Rate limiting between days
      if (i < options.daysBack - 1) {
        await delay(100);  // 100ms between days
      }
    }

    result.sessions_synced = sessionsTotal.length;
    result.success = result.days_synced > 0;

    return result;

  } catch (error) {
    result.error = error.message;
    return result;
  }
}

// Helper functions
function chunk<T>(array: T[], size: number): T[][] {
  const chunks: T[][] = [];
  for (let i = 0; i < array.length; i += size) {
    chunks.push(array.slice(i, i + size));
  }
  return chunks;
}

function delay(ms: number): Promise<void> {
  return new Promise(resolve => setTimeout(resolve, ms));
}
```

---

## ðŸ“‹ Migration Steps

### Step 1: Create Unified Function

```bash
# Create new function
mkdir -p supabase/functions/sync-all-google-fit
cp supabase/functions/sync-all-users-direct/index.ts \
   supabase/functions/sync-all-google-fit/index.ts

# Modify to include token refresh logic
# (Use code above)
```

### Step 2: Create Shared Token Manager

```typescript
// supabase/functions/_shared/google-token-manager.ts

import { SupabaseClient } from 'https://esm.sh/@supabase/supabase-js@2';

interface TokenData {
  access_token: string;
  refresh_token: string;
  expires_at: string;
}

export async function refreshToken(
  supabase: SupabaseClient,
  token: any
): Promise<TokenData> {
  const googleClientId = Deno.env.get('GOOGLE_CLIENT_ID')!;
  const googleClientSecret = Deno.env.get('GOOGLE_CLIENT_SECRET')!;

  // Call Google OAuth endpoint
  const response = await fetch('https://oauth2.googleapis.com/token', {
    method: 'POST',
    headers: { 'Content-Type': 'application/x-www-form-urlencoded' },
    body: new URLSearchParams({
      client_id: googleClientId,
      client_secret: googleClientSecret,
      refresh_token: token.refresh_token,
      grant_type: 'refresh_token',
    }),
  });

  if (!response.ok) {
    throw new Error(`Token refresh failed: ${response.status}`);
  }

  const data = await response.json();
  const now = new Date();
  const expiresAt = new Date(now.getTime() + (data.expires_in * 1000));

  // Update database
  await supabase
    .from('google_tokens')
    .update({
      access_token: data.access_token,
      refresh_token: data.refresh_token || token.refresh_token,
      expires_at: expiresAt.toISOString(),
      last_refreshed_at: now.toISOString(),
      refresh_count: (token.refresh_count || 0) + 1,
      updated_at: now.toISOString()
    })
    .eq('id', token.id);

  return {
    access_token: data.access_token,
    refresh_token: data.refresh_token || token.refresh_token,
    expires_at: expiresAt.toISOString()
  };
}

export async function getValidToken(
  supabase: SupabaseClient,
  userId: string,
  refreshThreshold: number = 5  // minutes
): Promise<string> {
  // Get token
  const { data: token, error } = await supabase
    .from('google_tokens')
    .select('*')
    .eq('user_id', userId)
    .eq('is_active', true)
    .single();

  if (error || !token) {
    throw new Error('No active token found');
  }

  // Check expiry
  const expiresAt = new Date(token.expires_at);
  const now = new Date();
  const minutesUntilExpiry = (expiresAt.getTime() - now.getTime()) / (1000 * 60);

  // Refresh if expiring soon
  if (minutesUntilExpiry < refreshThreshold) {
    const refreshed = await refreshToken(supabase, token);
    return refreshed.access_token;
  }

  return token.access_token;
}
```

### Step 3: Update Cron Job

```sql
-- Remove old cron jobs
SELECT cron.unschedule('refresh-google-tokens');
SELECT cron.unschedule('sync-google-fit-daily');

-- Create new unified cron job
SELECT cron.schedule(
  'sync-all-google-fit-unified',
  '*/10 * * * *',  -- Every 10 minutes
  $$
    SELECT net.http_post(
      url := 'https://[PROJECT].supabase.co/functions/v1/sync-all-google-fit',
      headers := jsonb_build_object(
        'Content-Type', 'application/json',
        'Authorization', 'Bearer [SERVICE_ROLE_KEY]'
      ),
      body := jsonb_build_object(
        'daysBack', 7,
        'batchSize', 10,
        'tokenRefreshThreshold', 30
      )
    ) AS request_id;
  $$
);
```

### Step 4: Update Manual Sync Functions

```typescript
// fetch-google-fit-data/index.ts

import { getValidToken } from '../_shared/google-token-manager.ts';

// Replace ~100 lines of token logic with:
const accessToken = await getValidToken(supabase, user.id);

// Then use accessToken for API calls
```

---

## ðŸ“Š Comparison

### Before (Current)

```
Cron Jobs: 2
â”œâ”€â”€ Token Refresh (every 15 min) â†’ 96 runs/day
â””â”€â”€ Data Sync (every 5 min) â†’ 288 runs/day
Total: 384 edge function calls/day

Edge Functions: 6
â”œâ”€â”€ fetch-google-fit-data (468 lines)
â”œâ”€â”€ sync-all-users-direct (371 lines)
â”œâ”€â”€ sync-historical-google-fit-data (476 lines)
â”œâ”€â”€ refresh-google-fit-token (77 lines)
â”œâ”€â”€ refresh-all-google-tokens (233 lines)
â””â”€â”€ store-google-token

Token Management: Duplicated in 3 places
```

### After (Proposed)

```
Cron Jobs: 1
â””â”€â”€ Unified Sync (every 10 min) â†’ 144 runs/day
Total: 144 edge function calls/day (62% reduction!)

Edge Functions: 4
â”œâ”€â”€ sync-all-google-fit (300 lines, unified)
â”œâ”€â”€ fetch-google-fit-data (250 lines, simplified)
â”œâ”€â”€ sync-google-fit-historical (300 lines, simplified)
â””â”€â”€ store-google-token

Token Management: Shared module (100 lines)
```

---

## âœ… Benefits Summary

### Performance
- âœ… **62% fewer edge function calls** (384 â†’ 144/day)
- âœ… **Atomic operations** (refresh â†’ use immediately)
- âœ… **Better rate limiting** (batch processing)

### Reliability
- âœ… **No race conditions** (token refreshed right before use)
- âœ… **Just-in-time refresh** (only when needed)
- âœ… **Automatic retry logic**

### Maintainability
- âœ… **Single cron job** (1 vs 2)
- âœ… **Shared token logic** (DRY principle)
- âœ… **Cleaner code** (~500 lines removed)

### Cost
- âœ… **Fewer API calls** to Google OAuth
- âœ… **Fewer database queries**
- âœ… **Lower edge function costs**

---

## ðŸš€ Implementation Timeline

### Phase 1: Foundation (2 hours)
1. Create `google-token-manager.ts` shared module
2. Create `sync-all-google-fit` function
3. Test with small user base

### Phase 2: Migration (1 hour)
1. Update cron jobs
2. Monitor for 24 hours
3. Verify all users syncing

### Phase 3: Cleanup (1 hour)
1. Simplify `fetch-google-fit-data`
2. Remove `refresh-all-google-tokens`
3. Update documentation

**Total: ~4 hours**

---

## ðŸŽ¯ Recommendation

**YES - Combine auto processes!**

This is a **clear win**:
- Simpler architecture
- Better performance
- More reliable
- Easier to maintain
- Lower costs

The unified approach is how it should have been designed from the start. Let's do it! ðŸš€

